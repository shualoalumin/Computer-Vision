{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인셉션 모듈을 만드는 데에 필요한 모듈 불러오기\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import plot_model # 모델 시각화\n",
    "\n",
    "# Naïve Inception 블록을 만들기 위한 함수\n",
    "def naive_inception(input_layer, conv1_filter, conv3_filter, conv5_filter):\n",
    "    # 1x1 사이즈의 kernel을 이용한 convolution2d layer\n",
    "    conv1 = keras.layers.Conv2D(conv1_filter, (1,1), padding='same', activation='relu')(input_layer)\n",
    "\n",
    "\n",
    "    # Q. 3x3 사이즈의 kernel을 이용한 convolution2d layer를 만들어 보세요.\n",
    "    conv3 = keras.layers.Conv2D(conv3_filter, (3,3), padding='same', activation='relu')(input_layer)\n",
    "    \n",
    "    # Q. 5x5 사이즈의 kernel을 이용한 convolution2d layer를 만들어 보세요.\n",
    "    conv5 = keras.layers.Conv2D(conv5_filter, (5,5), padding='same', activation='relu')(input_layer)\n",
    "    \n",
    "\n",
    "    # 3x3 max pooling layer (데이터의 가로 세로를 3x3로 살펴보고 가장 큰 값만 뽑아낸다)\n",
    "    pool = keras.layers.MaxPooling2D((3,3), strides=(1,1), padding='same')(input_layer)\n",
    "    # 위에서 언급한 4개의 layer 통해서 나온 feature map들을 모두 concatenation 한다.\n",
    "    out_layer = keras.layers.Concatenate()([conv1, conv3, conv5, pool])\n",
    "    return out_layer\n",
    "\n",
    "input_data = keras.layers.Input(shape=(256, 256, 3))\n",
    "naive_inception_out = naive_inception(input_data, 64, 128, 32)\n",
    "\n",
    "print(naive_inception_out)\n",
    "# 모델 만들기\n",
    "model = keras.models.Model(inputs=input_data, outputs=naive_inception_out)\n",
    "# 생성한 모델의 구조 확인하기\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 시각화하기\n",
    "plot_model(model, show_shapes=True, to_file='naive_inception_module.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 만드는 데에 필요한 모듈 불러오기\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import plot_model # 모델 시각화\n",
    "\n",
    "def residual_module(input_layer, n_filters):  \n",
    "\tmerge_input = input_layer\n",
    "\n",
    "# if문에서는 채널 사이즈가 동일한지 확인하고, 만일 동일하지 않다면 1x1 convolution을 통해서 채널 사이즈를 맞춰 준다.\n",
    "\tif input_layer.shape[-1] != n_filters:\n",
    "\t\tmerge_input = keras.layers.Conv2D(n_filters, (1,1), padding='same', activation='relu')(input_layer) # n_filter로 채널 사이즈를 맞춰 준다.\n",
    "\t# Conv2D layer\n",
    "\tconv1 = keras.layers.Conv2D(n_filters, (3,3), padding='same', activation='relu')(input_layer)\n",
    "\t# Conv2D layer\n",
    "\tconv2 = keras.layers.Conv2D(n_filters, (3,3), padding='same', activation='linear')(conv1)\n",
    "  # Add를 통해서 skip connection을 구현하는 부분\n",
    "\tout_layer = keras.layers.Add()([conv2, merge_input])\n",
    "\tout_layer = keras.layers.Activation('relu')(out_layer)\n",
    "\treturn out_layer\n",
    "input = keras.layers.Input(shape=(256, 256, 3))\n",
    "residual_out = residual_module(input, 64)\n",
    "# print(residual_out)\n",
    "# model = keras.models.Model(inputs=input, outputs=residual_out)\n",
    "# model.summary()\n",
    "plot_model(model, show_shapes=True, to_file='residual_module.png', dpi=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Class names and their corresponding indices in CIFAR-10\n",
    "class_names = ['aeroplane', 'bird', 'car', 'cat']\n",
    "class_indices = [0, 2, 1, 3]  # Indices for airplane, bird, car, cat\n",
    "\n",
    "# Base directory for saving images\n",
    "base_dir = 'c:/Users/User/Documents/GitHub/Computer-Vision/cifar_10_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Create directories and save images\n",
    "for class_name, class_index in zip(class_names, class_indices):\n",
    "    # Create class directories for training data\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    os.makedirs(train_class_dir, exist_ok=True)\n",
    "    \n",
    "    # Save training images\n",
    "    train_class_indices = np.where(y_train == class_index)[0]\n",
    "    for i, idx in enumerate(train_class_indices):\n",
    "        img_path = os.path.join(train_class_dir, f'{class_name}_{i}.png')\n",
    "        save_img(img_path, x_train[idx])\n",
    "    \n",
    "    # Create class directories for test data\n",
    "    test_class_dir = os.path.join(test_dir, class_name)\n",
    "    os.makedirs(test_class_dir, exist_ok=True)\n",
    "    \n",
    "    # Save test images\n",
    "    test_class_indices = np.where(y_test == class_index)[0]\n",
    "    for i, idx in enumerate(test_class_indices):\n",
    "        img_path = os.path.join(test_class_dir, f'{class_name}_{i}.png')\n",
    "        save_img(img_path, x_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir='c:/Users/User/Documents/GitHub/Computer-Vision/cifar_10_small/train'\n",
    "test_dir='c:/Users/User/Documents/GitHub/Computer-Vision/cifar_10_small/test'\n",
    "\n",
    "train_aeroplane_dir= os.path.join(train_dir,'aeroplane')\n",
    "train_bird_dir=os.path.join(train_dir,'bird')\n",
    "train_car_dir= os.path.join(train_dir,'car')\n",
    "train_cat_dir=os.path.join(train_dir,'cat')\n",
    "\n",
    "test_aeroplane_dir= os.path.join(test_dir,'aeroplane')\n",
    "test_bird_dir=os.path.join(test_dir,'bird')\n",
    "test_car_dir= os.path.join(test_dir,'car')\n",
    "test_cat_dir=os.path.join(test_dir,'cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 데이터셋의 이미지 개수 출력\n",
    "print('훈련용 aeroplane 이미지 전체 개수:', len(os.listdir(train_aeroplane_dir)))\n",
    "print('훈련용 bird 이미지 전체 개수:', len(os.listdir(train_bird_dir)))\n",
    "print('훈련용 car 이미지 전체 개수:', len(os.listdir(train_car_dir)))\n",
    "print('훈련용 cat 이미지 전체 개수:', len(os.listdir(train_cat_dir)))\n",
    "print('테스트용 aeroplane 이미지 전체 개수:', len(os.listdir(test_aeroplane_dir)))\n",
    "print('테스트용 bird 이미지 전체 개수:', len(os.listdir(test_bird_dir)))\n",
    "print('테스트용 car 이미지 전체 개수:', len(os.listdir(test_car_dir)))\n",
    "print('테스트용 cat 이미지 전체 개수:', len(os.listdir(test_cat_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data 파이프 라인 생성\n",
    "\n",
    "# 데이터를 디렉토리로부터 불러올 때, 한번에 가져올 데이터의 수\n",
    "batch_size=20\n",
    "\n",
    "# Training 데이터의 augmentation 파이프 라인 만들기\n",
    "augmentation_train_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1./255,  # 모든 데이터의 값을 1/255로 스케일 조정\n",
    "                                    rotation_range=40,  # 0~40도 사이로 이미지 회전\n",
    "                                    width_shift_range=0.2,  # 전체 가로 길이를 기준으로 0.2 비율까지 가로로 이동\n",
    "                                    height_shift_range=0.2,   # 전체 세로 길이를 기준으로 0.2 비율까지 가로로 이동\n",
    "                                    shear_range=0.2,  # 0.2 라디안 정도까지 이미지를 기울이기\n",
    "                                    zoom_range=0.2, # 확대와 축소의 범위 [1-0.2 ~ 1+0.2 ]\n",
    "                                    horizontal_flip=True,)  # 수평 기준 플립을 할 지, 하지 않을 지를 결정\n",
    "\n",
    "# Test 데이터의 augmentation 파이프 라인 만들기\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 4 classes.\n",
      "{'aeroplane': 0, 'bird': 1, 'car': 2, 'cat': 3}\n"
     ]
    }
   ],
   "source": [
    "# Augmentation 파이프라인을 기준으로 디렉토리로부터 데이터를 불러 오는 모듈 만들기\n",
    "train_generator = augmentation_train_datagen.flow_from_directory(\n",
    "        directory=train_dir, #  어느 디렉터리에서 이미지 데이터를 가져올 것인가?\n",
    "        target_size=(150, 150), # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
    "        batch_size=batch_size, # 디렉토리에서 batch size만큼의 이미지를 가져옵니다.\n",
    "        interpolation='bilinear',  # resize를 할 때, interpolatrion 기법을 결정합니다.\n",
    "        color_mode ='rgb',\n",
    "        shuffle='True', # 이미지를 셔플링할 지 하지 않을 지를 결정.\n",
    "        class_mode='categorical') # multiclass의 경우이므로 class mode는 categorical\n",
    "\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory=test_dir,  # 어느 디렉터리에서 이미지 데이터를 가져올 것인가?\n",
    "        target_size=(150, 150),  # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
    "        batch_size=batch_size,  # 디렉토리에서 batch size만큼의 이미지를 가져옵니다.\n",
    "        interpolation='bilinear',  # resize를 할 때, interpolation 기법을 결정합니다.\n",
    "        color_mode='rgb',\n",
    "        shuffle=False,  # 테스트 데이터는 셔플링하지 않습니다.\n",
    "        class_mode='categorical')  # multiclass의 경우이므로 class mode는 categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data의 파이프 라인이 batch size만큼의 데이터를 잘 불러오는 지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 데이터 크기: (20, 150, 150, 3)\n",
      "배치 레이블 크기: (20, 4)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator: \n",
    "    print('배치 데이터 크기:', data_batch.shape)\n",
    "    print('배치 레이블 크기:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바탕이 되는 Pretrained Model(ResNet50)을 불러오고 모델의 구조를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## back bone\n",
    "conv_base=tf.keras.applications.ResNet50(weights='imagenet',include_top=False)\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 모델을 구성합니다.\n",
    "\n",
    "input layer와 ResNet50 backbone, fully-connected layer를 연결하여 transfer learning 모델을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델 구성하기\n",
    "input_layer = tf.keras.layers.Input(shape=(150,150,3))\n",
    "x = conv_base(input_layer) # 위에서 불러온 pretrained model을 활용하기\n",
    "# 불러온 conv_base 모델의 최종 결과물은 Conv2D 연산의 feature map과 동일\n",
    "# 따라서 최종적인 Multiclass classfication을 하기 위해서는 Flatten을 해야 한다.\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "out_layer = tf.keras.layers.Dense(4, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv_base는 freeze 시킴으로써 이미 학습된 파라미터 값을 그대로 사용합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만들어진 모델의 구조를 살펴봅시다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_layer], outputs=[out_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_base의 trainable 값을 True로 설정하여 학습 가능한 파라미터를 활성화합니다.\n",
    "conv_base.trainable = True\n",
    "print(\"Trainable params with conv_base.trainable = True:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conv_base의 trainable 값을 False로 설정하여 학습 가능한 파라미터를 비활성화합니다.\n",
    "conv_base.trainable = False\n",
    "print(\"\\nTrainable params with conv_base.trainable = False:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=tf.keras.losses.categorical_crossentropy \n",
    "optimize=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "metric=tf.keras.metrics.categorical_accuracy\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimize,\n",
    "              metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_187']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 690ms/step - categorical_accuracy: 0.3536 - loss: 1.3754 - val_categorical_accuracy: 0.5307 - val_loss: 1.0800\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 66ms/step - categorical_accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_categorical_accuracy: 0.5307 - val_loss: 1.0800\n",
      "Epoch 3/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 550ms/step - categorical_accuracy: 0.4652 - loss: 1.1899 - val_categorical_accuracy: 0.5297 - val_loss: 1.0764\n",
      "Epoch 4/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 64ms/step - categorical_accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_categorical_accuracy: 0.5297 - val_loss: 1.0764\n",
      "Epoch 5/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 546ms/step - categorical_accuracy: 0.4793 - loss: 1.1651 - val_categorical_accuracy: 0.5322 - val_loss: 1.1229\n",
      "Epoch 6/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 64ms/step - categorical_accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_categorical_accuracy: 0.5322 - val_loss: 1.1229\n",
      "Epoch 7/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 565ms/step - categorical_accuracy: 0.4908 - loss: 1.1435 - val_categorical_accuracy: 0.5845 - val_loss: 1.0008\n",
      "Epoch 8/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 64ms/step - categorical_accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_categorical_accuracy: 0.5845 - val_loss: 1.0008\n",
      "Epoch 9/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 555ms/step - categorical_accuracy: 0.5007 - loss: 1.1310 - val_categorical_accuracy: 0.5888 - val_loss: 0.9811\n",
      "Epoch 10/20\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 67ms/step - categorical_accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_categorical_accuracy: 0.5888 - val_loss: 0.9811\n",
      "Epoch 11/20\n",
      "\u001b[1m 865/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1:09\u001b[0m 518ms/step - categorical_accuracy: 0.5056 - loss: 1.1269"
     ]
    }
   ],
   "source": [
    "history = model.fit( \n",
    "      train_generator,\n",
    "     steps_per_epoch=(len(os.listdir(train_aeroplane_dir)) + len(os.listdir(train_bird_dir)) + len(\n",
    "       os.listdir(train_car_dir)) + len(os.listdir(train_cat_dir))) // batch_size,\n",
    "      epochs=20,\n",
    "      validation_data=test_generator,\n",
    "      validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # 학습한 결과 시각화 \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43macc\u001b[49m))\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining acc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, val_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation acc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "# # 학습한 결과 시각화 \n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
